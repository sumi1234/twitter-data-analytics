{
 "metadata": {
  "name": "",
  "signature": "sha256:5ed4e7d3c77a895ee5508e123a72ddd014d871e2db6fc62cca9a7f77b519186f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Analytics in  Python \n",
      "\n",
      "In this notebook, we will discuss using python for analysis of twitter data.\n",
      "We will learn how to do a general analysis of twitter data e.g. word clouds, cluster analysis etc on an in-memory dataset.\n",
      "We will also discuss collecting data from the twitter API and the various ways we can store those tweets e.g. as a csv file, in a database (both SQL and NoSQL).\n",
      "At the end of this tutorial, we will also walk through building a real time twitter sentiment analysis web application from scratch using the things we have covered here.\n",
      "\n",
      "This notebook is based on my final year project in college on sentiment analysis.\n",
      "\n",
      "Note : There are rate limitations to the Twitter API and hence we can only get a very small number (about 1%) of streaming tweets.All of this analysis was done for personal/research purposes.\n",
      "\n",
      "Prerequisites : Basic programming knowledge.\n",
      "\n",
      "We will be using python throughout the tutorial. Each of these examples can easily be replicated in a different language but for the purpose of this tutorial we will be using Java.\n",
      "\n",
      "<h3><a id = 'first'>Data Mining Approaches</a></h3>\n",
      "<h3><a id = 'first'>Twitter Data Analytics</a></h3>\n",
      "<h3><a id = 'first'>Sentiment Analysis</a></h3>\n",
      "<h3><a id = 'first'>Building a web application</a></h3>\n",
      "<h3><a id = 'first'>End notes</a></h3>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Sentiment Analysis\n",
      "\n",
      "In this section, we will discuss how to do sentiment analysis of the collected twitter data. We will classify the tweet text as positive or negative based on the approach we use.\n",
      "Sentiment analysis is not very accurate in some cases but is a good exercise in analytics to start with. As far as I know, the most accurate sentiment analysis approach available currently is (add link to sentiment treebank java stanford link)\n",
      "In general, there are 2 approaches :\n",
      "1. Lexical Approach : Give each word a positive/negative score based on a word dictionary (AFFINE word list etc). Sum the sentiment scores of each word in the tweet and determine the overall sentiment of the tweet.\n",
      "\n",
      "2.Machine Learning Approach: In this approach, we use machine learning for classification of text data into positive or negative classes. However we need accurately labelled data for this approach. Examples include naive bayes classification, support vector machines etc.\n",
      "\n",
      "Personally, I prefer the second approach, but it requires good training data. \n",
      "The training dataset can be collected using either distance supervision(see this) or by using amazon mechanical turk for labelled data.\n",
      "The mechanics of mining and storing the data can be found here[1].The code for that can be easily changed to collect training data.\n",
      "\n",
      "\n",
      "\n",
      "###Preprocessing of Tweets\n",
      "\n",
      "We will discuss the second approach, assuming that we have training data in form of a csv file consisting of Text and Sentiment Columns. We will preprocess the raw tweets using the pandas and nltk packages available in python\n",
      "\n",
      "To follow along the tutorial, download the training and test datasets from here :\n",
      "sentiment140.com \n",
      "\n",
      "Something like this :\n",
      "\n",
      "Text, Sentiment\n",
      "this is a #tweet.Yay!!!, 4\n",
      "this is a #neg tweet :( , 0\n",
      "\n",
      "Assuming that positive sentiment is denoted by +4 and negative by 0.\n",
      "\n",
      "First step is to do preprocessing of data into a usable format for applying various classification algorithms. i.e. convert the tweet text into feature vectors. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "\n",
      "training_data = pd.read_csv(\"C:/Users/Sumedha/Documents/LiClipse Workspace/Tester/src/train_data.csv\")\n",
      "#training_data['processed_text'] = training_data.Text.apply(clean_text)\n",
      "#training_data['Sentiment'] = training_data.apply(sentiment_numeric)\n",
      "\n",
      "print training_data.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   Polarity                                               Text\n",
        "0         0  is upset that he can't update his Facebook by ...\n",
        "1         0  @Kenichan I dived many times for the ball. Man...\n",
        "2         0    my whole body feels itchy and like its on fire \n",
        "3         0  @nationwideclass no, it's not behaving at all....\n",
        "4         0                      @Kwesidei not the whole crew \n",
        "\n",
        "[5 rows x 2 columns]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first column is the id of the tweet. Polarity column represents the numeric sentiment of the tweet text.\n",
      "\n",
      "As you can see the tweet text is not uniform and contains punctuations, hashtags etc.\n",
      "\n",
      "To preprocess the data, we write a function called clean_text which strips the tweets of punctuations and extra spaces. And then apply that function to the Text column to clean the text in each row in the data frame.\n",
      "We also create a function called sentiment_numeric in case the sentiment given in the dataset is in text format"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_text(phrase):\n",
      "    clean_str = re.findall(r'\\w+', phrase, flags = re.UNICODE | re.LOCALE)\n",
      "    return ' '.join(clean_str).lower()\n",
      "\n",
      "def sentiment_numeric(phrase):\n",
      "    if phrase == \"positive\":\n",
      "        return 1\n",
      "    elif phrase == \"negative\":\n",
      "        return 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now repeat our previous code with the preprocessing done. Something like this :"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "\n",
      "training_data_2 = pd.read_csv(\"C:/Users/Sumedha/Documents/LiClipse Workspace/Tester/src/train.csv\")\n",
      "training_data_2['processed_text'] = training_data_2.Text.apply(clean_text)\n",
      "#training_data['Sentiment'] = training_data.apply(sentiment_numeric)\n",
      "\n",
      "print training_data.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   Polarity                                               Text\n",
        "0         0  is upset that he can't update his Facebook by ...\n",
        "1         0  @Kenichan I dived many times for the ball. Man...\n",
        "2         0    my whole body feels itchy and like its on fire \n",
        "3         0  @nationwideclass no, it's not behaving at all....\n",
        "4         0                      @Kwesidei not the whole crew \n",
        "\n",
        "[5 rows x 2 columns]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Till here preprocessing is the same.\n",
      "\n",
      "We now create a tf-idf vector[add link to tf-idf wiki here] of the training data. In this, we represent the tweets as numeric vectors.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words='english',\n",
      "                             min_df = 1,\n",
      "                             ngram_range = (0,3),\n",
      "                             use_idf = True)\n",
      "\n",
      "train_vectors = vectorizer.fit_transform(training_data_2.processed_text.as_matrix())\n",
      "\n",
      "train_target = training_data_2.Sentiment.as_matrix()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After this we will now train a classifier from the scikit-learn library on the train_vector. We can further extend this analysis by using different preprocessing steps and using different parameters for the classification task. \n",
      "In this post, I will show you how to use a naive bayes classifier, maximum entropy classifier and a linear SVM with default parameters. Further fine tuning can be done based on the data being used."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Naive Bayes Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "classifier = MultinomialNB().fit(train_vectors, train_target)\n",
      "\n",
      "cross_validation_scores = cross_val_score(classifier, train_vectors, train_target, cv = 5)\n",
      "\n",
      "print cross_validation_scores\n",
      "print cross_validation_scores.mean()\n",
      "print cross_validation_scores.std()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Unknown label type",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-8-027e593e399d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcross_validation_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Sumedha\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, class_prior)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mlabelbin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabelbin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabelbin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Sumedha\\Anaconda\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Sumedha\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindicator_matrix_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'multilabel-indicator'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\Sumedha\\Anaconda\\lib\\site-packages\\sklearn\\utils\\multiclass.pyc\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Unknown label type"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###References :\n",
      "\n",
      "4. Ravikrajan sentiment analysis\n",
      "5. laurent luce sentiment analysis\n",
      "6. stream hacker sentiment\n",
      "7 sentiment analysis of rotten tomatoes kaggle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Twitter Data Analytics"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "In this part of the tutorial, we will discuss analyzing twitter data by doing analysis of twitter data which has been stored in a database called TweetDB2 with collection name #worldcup.\n",
      "\n",
      "Data description : The data consists of over 20,000 tweets collected during the world cup final using the twitter streaming API."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}